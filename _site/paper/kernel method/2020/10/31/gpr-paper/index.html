
<!doctype html>














<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="gaussian process," />





  <link rel="alternate" href="/atom.xml" title="Sanghoon's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="논문 선정 강필성 교수님의 비즈니스 어낼리틱스 수업의 두번째 논문 구현 주제는 Kernel-based Leanring이다. Kernel-based Method 중 Gaussian Process를 이 기회를 통해 이해하고자 아래의 논문을 선정하였다.">
<meta name="keywords" content="gaussian process">
<meta property="og:type" content="article">
<meta property="og:title" content="Gaussian Process for Regression">
<meta property="og:url" content="http://localhost:4000/paper/kernel%20method/2020/10/31/gpr-paper/">
<meta property="og:site_name" content="Sanghoon's Blog">
<meta property="og:description" content="논문 선정 강필성 교수님의 비즈니스 어낼리틱스 수업의 두번째 논문 구현 주제는 Kernel-based Leanring이다. Kernel-based Method 중 Gaussian Process를 이 기회를 통해 이해하고자 아래의 논문을 선정하였다.">
<meta property="og:locale" content="en">
<meta property="og:image" content="/assets/figures/gpr/gpr0.png">
<meta property="og:image" content="/assets/figures/gpr/linear.png">
<meta property="og:image" content="/assets/figures/gpr/se.png">
<meta property="og:image" content="/assets/figures/gpr/periodic.png">
<meta property="og:image" content="/assets/figures/gpr/prior.png">
<meta property="og:image" content="/assets/figures/gpr/posterior.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gaussian Process for Regression">
<meta name="twitter:description" content="논문 선정 강필성 교수님의 비즈니스 어낼리틱스 수업의 두번째 논문 구현 주제는 Kernel-based Leanring이다. Kernel-based Method 중 Gaussian Process를 이 기회를 통해 이해하고자 아래의 논문을 선정하였다.">
<meta name="twitter:image" content="/assets/figures/gpr/gpr0.png">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://localhost:4000/"/>





  <title>Gaussian Process for Regression | Sanghoon's Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'G-XWS5Q99M4Y', 'auto');
  ga('send', 'pageview');
</script>













</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sanghoon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-sitemap">
          <a href="/navigator/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<div id="posts" class="posts-expand">
  
  

  

  
  
  

  <article class="post post-type- " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://localhost:4000/paper/kernel%20method/2020/10/31/gpr-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sanghoon Kim">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="assets/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sanghoon's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
          
          
            Gaussian Process for Regression
          
        </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-31T12:30:00+09:00">
                2020-10-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/#/paper" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
                  , 
                
              
                
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/category/#/kernel%20method" itemprop="url" rel="index">
                    <span itemprop="name">kernel method</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
            
                <div class="post-description">
                    
                </div>
            
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="논문-선정">논문 선정</h2>
<p>강필성 교수님의 비즈니스 어낼리틱스 수업의 두번째 논문 구현 주제는 <strong>Kernel-based Leanring</strong>이다. Kernel-based Method 중 Gaussian Process를
이 기회를 통해 이해하고자 아래의 논문을 선정하였다.<br /></p>

<div align="center">
<img src="/assets/figures/gpr/gpr0.png" title="Gaussian Process for Regression : A Quick Introduction" width="800" />
</div>

<blockquote>
  <p>Ebden, M. Gaussian Processes for Regression: A Quick Introduction (Robotics Research Group, University of Oxford, 2008)</p>
</blockquote>

<h2 id="motivation">MOTIVATION</h2>
<p>다음과 같이 독립 변수 \(x\)의 특정 값에서 종속 변수 \(y\)에 대해 노이즈가 있는 관측치가 주어졌을 때</p>

\[\mathcal{D} = [(\mathbf{x}_1, y_1) \dots (\mathbf{x}_n, y_n)]\]

\[\mathbf{x} \in \mathbb{R}^D\]

\[y \in \mathbb{R}\]

<p>일반적인 회귀 문제는 각 관측 값 \(y\)는 함수 \(f(x)\)와 가우시안 분포를 따르는 노이즈로 분해할 수 있다.</p>

\[y = f(x) + \epsilon\]

\[\epsilon \sim \mathcal{N}(0, \sigma_n^2)\]

<p>이를 통하여 새로운 데이터 포인트 \(x_*\)가 주어지면 해당하는 종속 변수 \(y_*\)에 대한 최적의 추정치를 찾아낼 수 있다.
만일 함수 \(f(x)\)가 선형임을 가정한다면 최소제곱법을 사용하여 최적의 추정치를 찾아내는 선 \(f(x)\)를 구할 수 있다 (선형 회귀).
하지만 대개의 경우 \(x\)와 \(y\)가 선형의 관계를 띈다는 선형 모델의 가정이 유효하지 않으므로 입력 데이터 \(x\)를 더 높은 차원의 공간에 투영한 다음 선형 모델을 적용한다.</p>

\[\mathbf{\phi(x)} \in \mathbb{R}^M \, ,where \, M&gt;D\]

\[y = f(\phi(x)) + \epsilon\]

\[\epsilon \sim \mathcal{N}(0, \sigma_n^2)\]

<p>예를 들어 \(\phi(x)\)를 2차, 3차 또는 nonpolynomial 함수를 사용하면 보다 고차원 공간에서 다항식 회귀를 수행할 수 있다.
하지만 이렇게 복잡한 함수의 회귀를 수행할 수 있게끔 하는 함수 \(f(\phi(x))\)를 어떻게 선택해야 할지의 문제가 남아 있다. 
가우시안 프로세스는 이 기저함수가 특정한 모델 (eg. \(f(x)=mx+c\)))을 가진다는 가정 대신에 데이터가 기저함수의 정확한 형태에 영향을 끼치도록 보다 일반적이고 유연한 방식으로 함수를 표현할 수 있게끔 한다. 
즉, 가우시안 프로세스는 기저함수를 명시적으로 지정할 필요가 없다.</p>

<h2 id="definition-of-a-gaussian-process">DEFINITION OF A GAUSSIAN PROCESS</h2>
<p>가우시안 프로세스는 다변량 가우시안 분포를 무한 차원으로 확장한 형태이다.
예를 들어 무한 차원의 벡터를 연속형 값을 인풋으로 받아 인덱싱된 값들을 반환하는 일종의 함수로 생각해보자.
이 개념을 무한 차원의 다변량 가우시안 분포에 적용하면 이것이 바로 가우시안 프로세스이다. <br /></p>

<p>가우시안 프로세스에서 얼핏 무한한 함수 공간상에서 분포를 고려하는 것이 쉽지 않아 보이지만 실제로 가우시안 프로세스는 Training Dataset과 
Test Dataset의 데이터 포인트들에 해당하는 입력 포인트 \(x_n\)의 유한 집합 내에서의 함숫값만을 고려하면 된다.
보다 쉽게 말하면 우리가 보유하고 있는 각 n개의 관측치 \(y = (y_1 \dots y_n)\)는 일부 \(n\) 변량의 다변량 가우시안 분포에서 
샘플링된 단일 점으로 생각할 수 있다. 이를 거꾸로 추론하는 것이 가우시안 프로세스이다. <br /></p>

<p>다변량 가우시안 분포는 단일 유한 차원 평균 벡터와 단일 유한 차원 공분산 행렬에 의해 완전히 지정되지만 
가우시안 프로세스에서는 정의된 유한 부분 집합에 대한 다변량 가우시안 분포가 여러 차원을 가질 수 있기 때문에 이를 활용할 수 없다.
대신 각 요소별 평균 함수 \(m(x)\)와 요소별 공분산 함수(커널 함수) \(k(x, x\prime)\)로 가우시안 프로세스를 나타낸다.</p>

\[m(x) = E[f(x)]\]

\[k(x, x\prime) = E[f(x_i)-m(x_i)(f(x_j)-m(x_j))]\]

<p>다시 앞선 일반적인 회귀 문제의 모형을 생각해보자. 
회귀 문제의 목표는 데이터가 주어졌을 때 이를 표현하는 어떤 함수 \(f(x)\)를 학습하고 찾아내는 것이다. 
가우시안 프로세스는 평균 함수와 공분산 함수를 통해 \(f(x)\)의 분포를 정의한다.</p>

\[f(x) \sim GP(m(x), k(x, x\prime))\]

<p>Training Dataset과 Test Dataset의 유한한 데이터 집합의 각 요소별 평균 벡터와 공분산 행렬은 이 \(m(x)\)와 
\(k(x, x\prime)\)의 요소별 값을 이용하여 쉽게 구할 수 있다. 즉, \(\mathbf{f} = (f_{\mathbf{x}_1}, \dots f_{\mathbf{x}_n})\)는 
\(\mathbf{f} \sim \mathcal{N}(\bar{\mathbf{f}}, K(X, X))\)으로 나타낼 수 있다.</p>

\[\bar{\mathbf{f}} = \begin{pmatrix} m(\mathbf{x}_1) \\ \vdots \\ m(\mathbf{x}_n) \end{pmatrix}\]

\[K(X, X) = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \ldots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \ldots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\]

<p>대부분의 경우 평균 함수로 얻을 수 있는 정보는 별로 없기에 가우시안 프로세스의 평균 \(m(x)\)를 단순한 설정을 위해 
0으로 가정하므로 하나의 관측치를 다른 관측치와 연결시켜 파라미터를 추론해야 하는 것은 공분산 함수 \(k(x, x\prime)\) 뿐이다.
논문에서는 공분산 함수의 형태를 Squared Exponential를 사용했고 이는 \(x\)와 \(x\prime\)이 유사한 값을 가질 수록 최대 허용 공분산 \(\sigma_f^2\)에 수렴하는 함수이다. 
즉, \(x\)와 \(x\prime\)이 유사하여 \(k(x, x\prime)\)이 최대 허용 공분산이 되면 \(f(x)\)와 \(f(x\prime)\)는 완벽한 상관관계를 지닌다고 해석할 수 있다. 
반대로 새로운 \(x\) 값이 추가될 때 먼 곳에 있는 관측값들은 큰 영향을 미칠 수 없도록 \(x\)와 \(x\prime\)이 멀어질 때 \(k(x, x\prime)\)은 0에 수렴하도록 구성되어 있다.</p>

\[k(x, x\prime) = \sigma_f^2 exp( - \frac{( x - x\prime)^2}{2l^2} )\]

<h2 id="reproduction">REPRODUCTION</h2>

<p>먼저 코드 구현을 위해 가우시안 프로세스 regression의 전체 골격을 잡아두자</p>

<h4 id="code1">code1</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GPR</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'L-BFGS-B'</span><span class="p">,</span> <span class="n">noise_var</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">noise_var</span> <span class="o">=</span> <span class="n">noise_var</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    
    <span class="k">def</span> <span class="nf">sample_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">sample_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">log_marginal_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">_cholesky_factorise</span><span class="p">(</span><span class="n">y_cov</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">_sample_multivariate_gaussian</span><span class="p">(</span><span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<h3 id="kernel-functions">Kernel functions</h3>
<p>다음으로 가우시안 프로세스의 구성요소 중 가장 중요한 커널 함수 \(k(x, x\prime)\)를 구현을 해보자. 논문에서 사용한 커널 함수인 Squared Exponential 외에 Linear Kernel과 Periodic Kernel을 모두 구현하였다.
각 커널 함수는 모두 symmetric positive semi-definite 공분산 행렬을 구성한다. 코드를 살펴보면 theta 변수와 bounds 변수를 통해 커널 파라미터 \(l\)과 \(\sigma_f^2\)를 조절하도록 하였다.
또한 요소별 공분산 행렬을 for 루프를 통해 구현할 수 있지만 보다 효율적으로 계산하기 위해 numpy 자료형에 최적화된 scipy의 pdist와 cdist를 통해 구현하였다. 
scipy 의 cdist 함수는 두 개의 자료형 A, B 를 받아서 AxB의 모든 페어에 대한 계산 결과를 2차원 배열로 리턴하고 pdist는 한 자료 안에서 객체 간의 pairwise distance를 리턴한다.</p>

<p><em>Linear</em> :</p>

\[k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2\mathbf{x}_i^T \mathbf{x}_j\]

<p><em>Squared Exponential</em> :</p>

\[k(\mathbf{x}_i, \mathbf{x}_j) = \text{exp} \left(\frac{-1}{2l^2} (\mathbf{x}_i - \mathbf{x}_j)^T (\mathbf{x}_i - \mathbf{x}_j)\right)\]

<p><em>Periodic</em> :</p>

\[k(\mathbf{x}_i, \mathbf{x}_j) = \text{exp}\left(-\sin(2\pi f(\mathbf{x}_i - \mathbf{x}_j))^T \sin(2\pi f(\mathbf{x}_i - \mathbf{x}_j))\right)\]

<h4 id="code2">code2</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span><span class="p">,</span> <span class="n">cdist</span><span class="p">,</span> <span class="n">squareform</span>
<span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signal_variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">signal_variance_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="n">signal_variance</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">signal_variance_bounds</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">X2</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X1</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">K</span>
     
<span class="k">class</span> <span class="nc">SquaredExponential</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="n">length_scale</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">length_scale_bounds</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">X2</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># K(X1, X1) is symmetric so avoid redundant computation using pdist.
</span>            <span class="n">dists</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">X1</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">metric</span><span class="o">=</span><span class="s">'sqeuclidean'</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">dists</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">squareform</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
            <span class="n">np</span><span class="p">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dists</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">X1</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">metric</span><span class="o">=</span><span class="s">'sqeuclidean'</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">dists</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">K</span>
       
<span class="k">class</span> <span class="nc">Periodic</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frequency</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">frequency_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">)):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="n">frequency</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">frequency_bounds</span><span class="p">]</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">X2</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># K(X1, X1) is symmetric so avoid redundant computation using pdist.
</span>            <span class="n">dists</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">xi</span><span class="p">,</span> <span class="n">xj</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span><span class="p">)).</span><span class="n">T</span><span class="p">,</span> 
                <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span><span class="p">))))</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dists</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">squareform</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
            <span class="n">np</span><span class="p">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dists</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">xi</span><span class="p">,</span> <span class="n">xj</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span><span class="p">)).</span><span class="n">T</span><span class="p">,</span> 
                <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span><span class="p">))))</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dists</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">K</span>
</code></pre></div></div>

<h3 id="sampling-from-the-gp-prior">Sampling from the GP prior</h3>
<p>가우시안 프로세스에서 함수를 샘플링하려면 먼저 샘플링된 함수를 평가할 입력 지점 \(n_*\)을 정해주고 해당하는 \(n_*\) 변량의 다변량 가우시안 분포에서 추출해야 한다.
이는 아직 관찰된 데이터를 고려하지 않았기 때문에 커널 함수에 대한 사전 정보가 부족한 가우시안 프로세스 사전 분포에서의 추출을 뜻한다.</p>

\[\mathbf{f}_* \sim \mathcal{N}\left(\mathbf{0}, K(X_*, X_*)\right).\]

<p>또한 앞선 언급하였듯 가우시안 프로세스의 평균 함수 \(m(x)\)는 평균 함수로부터 얻을 수 있는 정보가 별로 없기에 단순한 설정을 위해 0으로 가정하였다. 그리고 \(X_*\)로부터 각 커널함수를 적용하여
공분산 행렬 \(K(X_*, X_*)\)를 각각 구성한다. 각 커널 함수는 모두 symmetric positive semi-definite 공분산 행렬을 구성하므로 Cholesky decomposition를 통해 이를 분해할 수 있다.</p>

\[K(X_*, X_*)=LL^T\]

<p>다음으로 Cholesky decomposition의 분해 결과에서 가우시안 분포의 샘플 \(\mathbf{z} \sim \mathcal{N}(\mathbf{m}, K)\)를 생성하기 위해 아래의 공식을 활용한다.</p>

\[\mathbf{u} \sim \mathcal{N}(\mathbf{0}, I)\]

\[\mathbf{z}=\mathbf{m} + L\mathbf{u}\]

\[\mathbb{E}[\mathbf{z}] = \mathbf{m} + L\mathbb{E}[\mathbf{u}] = \mathbf{m}\]

\[\text{cov}[\mathbf{z}] = L\mathbb{E}[\mathbf{u}\mathbf{u}^T]L^T = LL^T = K\]

<p>이를 코드를 통해 구현하면 아래와 같다.</p>

<h4 id="code3">code3</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">sample_prior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_cov</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">y_cov</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">epsilon</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">y_cov</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y_mean</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_mean</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>

<p>이제 앞서 정의한 세 개의 커널 함수는 각각 다른 가우시안 프로세스 사전분포를 가지므로 이에 따른 각 커널 함수별 가우시안 프로세스 사전분포에서 추출한 샘플들을 추출하면 아래와 같다.</p>

<h4 id="code4">code4</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GPR</span><span class="p">.</span><span class="n">sample_prior</span> <span class="o">=</span> <span class="n">sample_prior</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span> 

<span class="n">sigma_f_sq</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Linear signal_variance
</span><span class="n">l</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Squared Exponential length_scale
</span><span class="n">f</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Periodic frequency
</span>
<span class="n">gps</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Linear'</span><span class="p">:</span> <span class="n">GPR</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">sigma_f_sq</span><span class="p">)),</span> 
       <span class="s">'SquaredExponential'</span><span class="p">:</span> <span class="n">GPR</span><span class="p">(</span><span class="n">SquaredExponential</span><span class="p">(</span><span class="n">l</span><span class="p">)),</span>
       <span class="s">'Periodic'</span><span class="p">:</span> <span class="n">GPR</span><span class="p">(</span><span class="n">Periodic</span><span class="p">(</span><span class="n">f</span><span class="p">))}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">gp</span> <span class="ow">in</span> <span class="n">gps</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">y_samples</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">sample_prior</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_samples</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'{} kernel'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<div align="center">
<img src="/assets/figures/gpr/linear.png" title="Linear Kernel" width="500" />
</div>

<div align="center">
<img src="/assets/figures/gpr/se.png" title="Squared Exponential Kernel" width="500" />
</div>

<div align="center">
<img src="/assets/figures/gpr/periodic.png" title="Periodic Kernel" width="500" />
</div>

<p>그려진 plot은 각 커널 함수에 해당하는 가우시안 프로세스 사전분포 별로 5개의 함수를 가져온 뒤 이를 그린 결과물이다.
의미있는 예측을 하려면 이렇게 사전 분포로부터 생성될 수 있는 함수 중 관측된 데이터와 일치하는 함수만을 포함하도록 제한해야 한다. 
이 과정은 가우시안 프로세스 사후분포로부터 샘플링하는 과정을 통해 이루어 진다. <br /></p>

<h3 id="sampling-from-the-gp-posterior">Sampling from the GP posterior</h3>
<p>먼저 가우시안 프로세스 사전분포 하에서 관측치 y는 다음과 같이 정의될 수 있다.</p>

\[\mathbf{y} \sim \mathcal{N}\left(\mathbf{0}, K(X, X) + \sigma_n^2I\right)\]

<p>추가된 항인 \(\sigma_n^2I\)는 제일 처음 언급한 내용대로 관측치의 가우시안 분포를 따르는 노이즈다. 노이즈는 각 관측치에 대해 독립이고 동일한 분포로부터 나온 값이므로 \(K(X, X)\)의 대각 요소에만 추가된다. 
다음으로 다변량 가우시안 분포의 marginalisation property를 사용하여 가우시안 사전분포를 따르는 \(\mathbf{f_*}\)와 관측치 \(\mathbf{y}\)의 결합분포를 구하면 다음과 같다.</p>

\[\begin{bmatrix} \mathbf{y} \\ \mathbf{f}_* \end{bmatrix} = \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} K(X, X)  + \sigma_n^2I &amp;&amp; K(X, X_*) \\ K(X_*, X) &amp;&amp; K(X_*, X_*)\end{bmatrix}\right)\]

<p>가우시안 프로세스의 사후 분포는 이 결합분포가 조건부로 주어졌을 때의 \(\mathbf{f}_*\)의 분포이고 다음과 같이 정의된다.</p>

\[\mathbf{f}_* | X_*, X, \mathbf{y} \sim \mathcal{N}\left(\bar{\mathbf{f}}_*, \text{cov}(\mathbf{f}_*)\right),\]

\[\bar{\mathbf{f}}_* = K(X_*, X)\left[K(X, X) + \sigma_n^2\right]^{-1}\mathbf{y}\]

\[\text{cov}(\mathbf{f}_*) = K(X_*, X_*) - K(X_*, X)\left[K(X, X) + \sigma_n^2\right]^{-1}K(X, X_*)\]

<p>앞서 사전 분포로부터 샘플링한 방식과 전체적으로 코드의 구성은 동일하지만 사전 분포의 평균과 공분산 대신 사후 분포의 평균과 공분산을 사용한다. 또한 주어진 데이터로부터 얻어지는 고정된 부분을 \(\mathbf{\alpha}\)와 
\(\mathbf{v}\)로 정의하고 미리 계산해둠으로써 코드의 구성을 간결하게 했다.</p>

\[[K(X, X) + \sigma_n^2] = L L^T\]

\[\mathbf{\alpha} = \left[K(X, X) + \sigma_n^2\right]^{-1}\mathbf{y} = L^T \backslash(L \backslash \mathbf{y})\]

\[\mathbf{v} = L^T [K(X, X) + \sigma_n^2]^{-1}K(X, X_*) = L \backslash K(X, X_*)\]

\[\bar{\mathbf{f}}_* = K(X, X_*)^T\mathbf{\alpha}\]

\[\text{cov}(\mathbf{f}_*) = K(X_*, X_*) - \mathbf{v}^T\mathbf{v}\]

<h4 id="code5">code5</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    
    <span class="c1"># compute alpha
</span>    <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">noise_var</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    
    <span class="c1"># Compute posterior mean
</span>    <span class="n">K_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">K_trans</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
   
    <span class="c1"># Compute posterior covariance
</span>    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">K_trans</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># L.T * K_inv * K_trans.T
</span>    <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    
    <span class="n">y_cov</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">y_cov</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">epsilon</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_cholesky_factorise</span><span class="p">(</span><span class="n">y_cov</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y_mean</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_mean</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
</code></pre></div></div>

<p>관측치를 커널 함수가 Squared Exponential인 가우시안 프로세스 사전분포로부터 랜덤하게 10개를 샘플링한 후, 가우시안 프로세스 사후분포를 정의하여 샘플링하면 아래의 결과와 같다.</p>

<h4 id="code6">code6</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GPR</span><span class="p">.</span><span class="n">sample_posterior</span> <span class="o">=</span> <span class="n">sample_posterior</span>

<span class="n">gp</span> <span class="o">=</span> <span class="n">gps</span><span class="p">[</span><span class="s">'SquaredExponential'</span><span class="p">]</span>

<span class="c1"># 사전분포로부터 Data generation
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span> 
<span class="n">y_train</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">sample_prior</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> 
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="s">'r+'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Observations'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<div align="center">
<img src="/assets/figures/gpr/prior.png" title="Sampling from the GP prior" width="500" />
</div>

<h4 id="code7">code7</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f_star_samples</span><span class="p">,</span> <span class="n">f_star_mean</span><span class="p">,</span> <span class="n">f_star_covar</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">sample_posterior</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">pointwise_variances</span> <span class="o">=</span> <span class="n">f_star_covar</span><span class="p">.</span><span class="n">diagonal</span><span class="p">()</span>
<span class="n">error</span> <span class="o">=</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pointwise_variances</span><span class="p">)</span> <span class="c1"># 95% confidence interval
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_star_mean</span><span class="p">,</span> <span class="s">'b'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">f_star_mean</span> <span class="o">-</span> <span class="n">error</span><span class="p">,</span> <span class="n">f_star_mean</span> <span class="o">+</span> <span class="n">error</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot samples from posterior
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">f_star_samples</span><span class="p">)</span>

<span class="c1"># Also plot our observations for comparison
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="s">'r+'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Posterior samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<div align="center">
<img src="/assets/figures/gpr/posterior.png" title="Sampling from the GP posterior" width="500" />
</div>

<p>그려진 plot은 사후분포로부터 생성된 함수의 95% 신뢰구간을 나타낸다. 이 plot으로부터 기존의 관측치에서 새로운 데이터가 멀리 벗어날수록 예측은 사전 분포에 대한 영향을 잃고 함수 값의 분산이 증가하는 것을 알 수 있다. 
또한 사후분포로부터 생성된 함수들은 모두 관측치를 지나는 것처럼 보이는데 이는 관측치에 대한 노이즈를 매우 작은 값(\(10^{-8}\))으로 설정했기 때문이다. 이 관측치에 대한 노이즈 값 부분은 코드 구성을 살펴보면 알 수 있듯이 직접 설정하여 바꿀 수도 있다.</p>

<p>이렇게 관측치를 사전분포로부터 추출하면 당연히 사전분포를 형성하는 커널 함수가 데이터에 적합한 커널 함수가 될 수 밖에 없다. 
그러나 실제 관측치는 이렇게 정의된 사전분포에 딱 떨어진 값이 아니다. 그래서 최적의 커널 함수를 찾는 것 역시 가우시안 프로세스의 중요한 과제이다.
이는 이 포스트의 가장 앞 단락에서 설명한 함수 \(f(\phi(x))\)를 어떤 함수로 가져가야 주어진 데이터를 가장 잘 설명할 수 있을까에 대한 문제와 비슷하다. 
하지만 가우시안 프로세스로부터 생성된 커널 함수의 집합은 기저함수의 집합보다 훨씬 광범위한 함수 분포를 포함하기 때문에 최적의 커널 함수를 찾지 못하는 것은 최적의 기저함수를 찾지 못하는 것보다 상대적으로 덜 위험한 결과를 보인다.</p>

<p>다음으로 커널 함수를 잘 선정했더라도 커널 파라미터를 어떻게 결정해야 할지의 문제가 남아 있다. Squared Exponential 커널 함수에서도 커널 파라미터 \(l\)과 \(\sigma_f^2\)를 조절함에 따라 결과가 달리 나타난다.
이는 베이즈 정리를 통하여 해결할 수 있다. 베이즈 정리에 의해 커널 파라미터 \(\theta\)에 대한 사후 분포는 다음과 같이 정의된다.</p>

\[p(\pmb{\theta}|\mathbf{y}, X) = \frac{p(\mathbf{y}|X, \pmb{\theta}) p(\pmb{\theta})}{p(\mathbf{y}|X)}.\]

<p>\(\theta\)에 대한 maximum a posteriori (MAP)는 해당 사후 분포 \(p(\pmb{\theta}|\mathbf{y}, X)\)가 가장 클 때 발생한다. 일반적으로 커널 파라미터는 사전 정보가 거의 없기 때문에 커널 파라미터에 대한 사전분포는 uniform 분포를 가정한다.
이 경우 \(\theta_{MAP}\)는 Marginal Likelihood를 최대화하여 구할 수 있다. 실제로 연산을 할 때는 편의를 위해 Log Marginal Likelihood를 최대화하는 \(\theta\)가 \(\theta_{MAP}\)이다.</p>

\[p(\mathbf{y}|X, \pmb{\theta}) = \mathcal{N}(\mathbf{0}, K(X, X) + \sigma_n^2I)\]

\[\text{log}p(\mathbf{y}|X, \pmb{\theta}) = -\frac{1}{2}\mathbf{y}^T\left[K(X, X) + \sigma_n^2I\right]^{-1}\mathbf{y} - \frac{1}{2}\text{log}\lvert K(X, X) + \sigma_n^2I \lvert - \frac{n}{2}\text{log}2\pi\]

<p>앞선 사후 분포 파트의 코드 구성에서 Log Marginal Likelihood 구성 식 중 데이터로부터 얻어지는 고정 값 \(\mathbf{\alpha}\) 부분은 미리 계산해두었다. (\(\mathbf{\alpha} = \left[K(X, X) + \sigma_n^2\right]^{-1}\mathbf{y} = L^T \backslash(L \backslash \mathbf{y})\))
남은 부분은 Cholesky decomposition을 통해 \([K(X, X) + \sigma_n^2] = L L^T\)로 분해하여 아래와 같이 전개함으로써 계산을 편리하게 바꾼다.</p>

\[\lvert K(X, X) + \sigma_n^2 \lvert = \lvert L L^T \lvert = \prod_{i=1}^n L_{ii}^2 \quad \text{or} \quad \text{log}\lvert{K(X, X) + \sigma_n^2}\lvert = 2 \sum_i^n \text{log}L_{ii}\]

<p>이를 코드를 통해 구성하면 다음과 같다. 코드 구성 상 편의를 위해 log marginal likelihood에 -1을 곱하여 이를 최소화시키는 파라미터를 찾도록 수정하였다.</p>

<h4 id="code8">code8</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_marginal_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">noise_var</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="n">noise_var</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise_var</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">noise_var</span>
    
    <span class="c1"># Build K(X, X)
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>    
    <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">noise_var</span>
       
    <span class="c1"># Compute L and alpha for this K (theta).
</span>    <span class="n">L</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_cholesky_factorise</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
        
    <span class="c1"># Compute log marginal likelihood.
</span>    <span class="n">log_likelihood</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">log_likelihood</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">L</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">log_likelihood</span> <span class="o">-=</span> <span class="n">K</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">log_likelihood</span>

<span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">obj_func</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
            <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
  
    <span class="n">results</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> 
                       <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> 
                       <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> 
                       <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">,</span> 
                       <span class="n">jac</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                       <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">bounds</span><span class="p">)</span>

    <span class="c1"># Store results of optimization.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">max_log_marginal_likelihood_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">results</span><span class="p">[</span><span class="s">'fun'</span><span class="p">]</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">.</span><span class="n">theta_MAP</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="s">'success'</span><span class="p">]</span>

<span class="n">success</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>
<p>이제 Squared Exponential 커널 함수의 커널 파라미터 \(\pmb{\theta}=\{l\}\)에 대한 \(\pmb{\theta}_{MAP}\)를 계산하면 \(\pmb{\theta}_{MAP}\)는 1.31723513이고 Maximised log marginal liklehihood는 1.32706104로 나타난다.
물론 이 값이 전역 최적해라 할 수는 없다. 하지만 \(\pmb{\theta}_{MAP}\)는 일반적으로 좋은 추정치이며 데이터를 생성하는데 사용되는 \(\pmb{\theta}\)에 매우 가까운 값이라는 것을 알 수 있다. 
또한 위의 코드에서는 보다 간단한 예시를 위해 \(l\)에 대해서만 추정을 진행하기 위해 관측치의 노이즈에 대한 값을 고정 값(\(10^{-8}\))으로 두었지만 커널 파라미터 \(\sigma_n^2\)에 대해서도 동일한 과정을 적용하면 \(\sigma_n^2\)의 추정치를 구할 수 있다.</p>

<h2 id="결론">결론</h2>
<p>이번 논문 구현 과제를 통해 가우시안 프로세스를 회귀 문제에 적용하는 과정을 설명했다. 하지만 이는 가우시안 프로세스에 대한 단적인 부분에 불과하다. 
가우시안 프로세스는 현재 연구가 깊게 이루어진 분야로 오늘 소개한 내용은 기초에 불과하고 이러한 회귀 문제에 적용하는 과정뿐 아니라 분류 문제에 적용하는 과정, 딥러닝 모델에 결합하는 방법론 등 더욱 다양한 부분이 남아 있다.
아직 가우시안 프로세스에 대한 이해가 부족하기에 회귀 문제에 적용하는 과정에 대해서도 모든 내용을 잘 담지는 못 하였으나, 추후 공부를 통해 보다 깊이 있는 내용을 추가적으로 더 다뤄보고 싶다.</p>

<blockquote>
  <p><strong>참고문헌</strong></p>
  <ol>
    <li>Williams, C. K., &amp; Rasmussen, C. E. (1996). Gaussian processes for regression. In Advances in neural information processing systems (pp. 514-520).<br /></li>
    <li>Ebden, M. (2015). Gaussian processes: A quick introduction. arXiv preprint arXiv:1505.02965.</li>
  </ol>
</blockquote>


      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            
            <a href="/tag/#/gaussian%20process" rel="tag"># gaussian process</a>
          
        </div>
      

      
      
      
      
      

      
      
        <div class="post-nav" id="post-nav-id">
          <div class="post-nav-next post-nav-item">
            
              <a href="/paper/anomaly%20detection/2020/11/15/anogan-paper/" rel="next" title="Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery">
                <i class="fa fa-chevron-left"></i> Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/paper/openset/2020/10/11/openmax-paper/" rel="prev" title="Towards open set deep networks">
                Towards open set deep networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      
      

      
    </footer>
  </article>

  <div class="post-spread">
    
  </div>
</div>


<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = http://localhost:4000/paper/kernel%20method/2020/10/31/gpr-paper/;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = /paper/kernel%20method/2020/10/31/gpr-paper; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://october25kim.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        
        
        




      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/assets/images/avatar.gif"
               alt="Sanghoon Kim" />
          <p class="site-author-name" itemprop="name">Sanghoon Kim</p>
           
              <p class="site-description motion-element" itemprop="description">Study Record of Machine Learning, Deep Learning</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            





            
              <div class="post-toc-content">
    <ol class=nav>
      <li class="nav-item nav-level-2"> <a class="nav-link" href="#논문-선정"> <span class="nav-number">1</span> <span class="nav-text">논문 선정</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#motivation"> <span class="nav-number">2</span> <span class="nav-text">MOTIVATION</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#definition-of-a-gaussian-process"> <span class="nav-number">3</span> <span class="nav-text">DEFINITION OF A GAUSSIAN PROCESS</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#reproduction"> <span class="nav-number">4</span> <span class="nav-text">REPRODUCTION</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-3"> <a class="nav-link" href="#kernel-functions"> <span class="nav-number">4.1</span> <span class="nav-text">Kernel functions</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code2"> <span class="nav-number">4.1.1</span> <span class="nav-text">code2</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#sampling-from-the-gp-prior"> <span class="nav-number">4.2</span> <span class="nav-text">Sampling from the GP prior</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code3"> <span class="nav-number">4.2.1</span> <span class="nav-text">code3</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code4"> <span class="nav-number">4.2.2</span> <span class="nav-text">code4</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-3"> <a class="nav-link" href="#sampling-from-the-gp-posterior"> <span class="nav-number">4.3</span> <span class="nav-text">Sampling from the GP posterior</span> </a> <ol class="nav-child"> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code5"> <span class="nav-number">4.3.1</span> <span class="nav-text">code5</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code6"> <span class="nav-number">4.3.2</span> <span class="nav-text">code6</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code7"> <span class="nav-number">4.3.3</span> <span class="nav-text">code7</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li> <li class="nav-item nav-level-4"> <a class="nav-link" href="#code8"> <span class="nav-number">4.3.4</span> <span class="nav-text">code8</span> </a> <ol class="nav-child"> <ol class="nav-child"> </li></ol> </li></ol> </li></ol> </li></ol> </li> <li class="nav-item nav-level-2"> <a class="nav-link" href="#결론"> <span class="nav-number">5</span> <span class="nav-text">결론</span> </a> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child"> <ol class="nav-child">
    </ol>
  </div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sanghoon Kim</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://jekyllrb.com">Jekyll</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  




  

    

  





  






  

  

  
  


  
  


  

  

</body>
</html>

